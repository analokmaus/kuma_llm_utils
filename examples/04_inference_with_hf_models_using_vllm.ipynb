{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kuma_llm_utils.llm import vLLMEngineAsync, vLLMWorkerAsync, vLLMVisionWorkerAsync "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-28 18:40:38 config.py:510] This model supports multiple tasks: {'generate', 'embed', 'reward', 'score', 'classify'}. Defaulting to 'generate'.\n",
      "WARNING 01-28 18:40:38 config.py:588] bitsandbytes quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 01-28 18:40:39 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='google/gemma-2-2b-it', speculative_config=None, tokenizer='google/gemma-2-2b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=google/gemma-2-2b-it, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 01-28 18:40:40 selector.py:120] Using Flash Attention backend.\n",
      "INFO 01-28 18:40:40 model_runner.py:1094] Starting to load model google/gemma-2-2b-it...\n",
      "INFO 01-28 18:40:41 loader.py:1039] Loading weights with BitsAndBytes quantization.  May take a while ...\n",
      "INFO 01-28 18:40:41 weight_utils.py:251] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eaabd65fe7f45df8d9b4430e461c0bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-28 18:40:42 model_runner.py:1099] Loading model weights took 2.0921 GB\n",
      "INFO 01-28 18:40:44 worker.py:241] Memory profiling takes 1.60 seconds\n",
      "INFO 01-28 18:40:44 worker.py:241] the current vLLM instance can use total_gpu_memory (47.50GiB) x gpu_memory_utilization (0.40) = 19.00GiB\n",
      "INFO 01-28 18:40:44 worker.py:241] model weights take 2.09GiB; non_torch_memory takes 0.16GiB; PyTorch activation peak memory takes 2.36GiB; the rest of the memory reserved for KV Cache is 14.39GiB.\n",
      "INFO 01-28 18:40:44 gpu_executor.py:76] # GPU blocks: 9068, # CPU blocks: 2520\n",
      "INFO 01-28 18:40:44 gpu_executor.py:80] Maximum concurrency for 8192 tokens per request: 17.71x\n",
      "INFO 01-28 18:40:46 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:19<00:00,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-28 18:41:05 model_runner.py:1535] Graph capturing finished in 20 secs, took 2.72 GiB\n",
      "INFO 01-28 18:41:05 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 23.24 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-28 18:41:14 config.py:510] This model supports multiple tasks: {'generate', 'embed', 'reward', 'score', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 01-28 18:41:14 llm_engine.py:234] Initializing an LLM engine (v0.6.6.post1) with config: model='Qwen/Qwen2-VL-2B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2-VL-2B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2-VL-2B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"candidate_compile_sizes\":[],\"compile_sizes\":[],\"capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 01-28 18:41:15 selector.py:120] Using Flash Attention backend.\n",
      "INFO 01-28 18:41:15 model_runner.py:1094] Starting to load model Qwen/Qwen2-VL-2B-Instruct...\n",
      "WARNING 01-28 18:41:15 utils.py:624] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.\n",
      "INFO 01-28 18:41:15 weight_utils.py:251] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cee63828461e473782cbb6ae08a142e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-28 18:41:16 model_runner.py:1099] Loading model weights took 2.3272 GB\n",
      "INFO 01-28 18:41:29 worker.py:241] Memory profiling takes 12.93 seconds\n",
      "INFO 01-28 18:41:29 worker.py:241] the current vLLM instance can use total_gpu_memory (47.50GiB) x gpu_memory_utilization (0.40) = 19.00GiB\n",
      "INFO 01-28 18:41:29 worker.py:241] model weights take 2.33GiB; non_torch_memory takes 0.15GiB; PyTorch activation peak memory takes 3.29GiB; the rest of the memory reserved for KV Cache is 13.23GiB.\n",
      "INFO 01-28 18:41:30 gpu_executor.py:76] # GPU blocks: 30972, # CPU blocks: 9362\n",
      "INFO 01-28 18:41:30 gpu_executor.py:80] Maximum concurrency for 32768 tokens per request: 15.12x\n",
      "INFO 01-28 18:41:31 model_runner.py:1415] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:18<00:00,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-28 18:41:50 model_runner.py:1535] Graph capturing finished in 19 secs, took 2.56 GiB\n",
      "INFO 01-28 18:41:50 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 33.42 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vllm_engine = vLLMEngineAsync(\n",
    "    vllm_params=dict(\n",
    "        model='google/gemma-2-2b-it',\n",
    "        dtype='bfloat16',\n",
    "        gpu_memory_utilization=0.4,\n",
    "        quantization='bitsandbytes',\n",
    "        load_format='bitsandbytes',\n",
    "        trust_remote_code=True,\n",
    "        disable_log_requests=True\n",
    "    )\n",
    ")\n",
    "vllm_engine_vision = vLLMEngineAsync(\n",
    "    vllm_params=dict(\n",
    "        model='Qwen/Qwen2-VL-2B-Instruct',\n",
    "        dtype='bfloat16',\n",
    "        gpu_memory_utilization=0.4,\n",
    "        quantization='fp8',\n",
    "        trust_remote_code=True,\n",
    "        disable_log_requests=True\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task: text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_prompt = '''\\\n",
    "Answer to the following questions about {moutain}:\n",
    "1. {question1}\n",
    "2. {question2}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vllm_worker = vLLMWorkerAsync(\n",
    "    engine=vllm_engine,\n",
    "    prompt_template=sample_prompt,\n",
    "    generation_params=dict(\n",
    "        temperature=0.0,\n",
    "        top_p=1.0,\n",
    "        max_tokens=1024)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-28 18:41:51 metrics.py:467] Avg prompt throughput: 0.8 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-28 18:41:51,524 - 77   - INFO     - vLLMEngineAsync |  input tokens = 37 | output tokens = 67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the answers to your questions about Mount Fuji:\n",
      "\n",
      "1. **Which country is the mountain located?**  Mount Fuji is located in **Japan**. \n",
      "2. **How tall is the mountain?** Mount Fuji is **3,776 meters (12,388 feet)** tall.\n"
     ]
    }
   ],
   "source": [
    "print(await vllm_worker.generate([\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'question1': 'Which country is the mountain located?',\n",
    "        'question2': 'How tall is the mountain?',\n",
    "        'moutain': 'Mount Fuji'\n",
    "    }]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-28 18:41:51,966 - 77   - INFO     - vLLMEngineAsync |  input tokens = 90 | output tokens = 55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the answers to your questions about Mount Kilimanjaro:\n",
      "\n",
      "1. **Which country is the mountain located?**  Tanzania\n",
      "2. **How tall is the mountain?** 5,895 meters (19,341 feet)\n"
     ]
    }
   ],
   "source": [
    "print(await vllm_worker.generate([\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'question1': 'Which country is the mountain located?',\n",
    "        'question2': 'How tall is the mountain?',\n",
    "        'moutain': 'Mt. Everest'}, \n",
    "    {\n",
    "        'role': 'assistant',\n",
    "        'text': '1. Nepal\\n2. 8848m'\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'question1': 'Which country is the mountain located?',\n",
    "        'question2': 'How tall is the mountain?',\n",
    "        'moutain': 'Mt. Kilimanjaro'\n",
    "    },\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task: multimodal + few-shot generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_prompt = '''\\\n",
    "Answer to the following questions based on the attached image, and output the answer in JSON format:\n",
    "1. {question1}\n",
    "2. {question2}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vllm_vision_worker = vLLMVisionWorkerAsync(\n",
    "    engine=vllm_engine_vision,\n",
    "    prompt_template=sample_prompt,\n",
    "    generation_params=dict(\n",
    "        temperature=0.0,\n",
    "        top_p=0.1,\n",
    "        max_tokens=1024),\n",
    "    image_max_size=768\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-28 18:41:57 metrics.py:467] Avg prompt throughput: 148.3 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-28 18:41:57,796 - 77   - INFO     - vLLMEngineAsync |  input tokens = 1017 | output tokens = 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Mount Fuji\n",
      "2. 3776m\n"
     ]
    }
   ],
   "source": [
    "print(await vllm_vision_worker.generate([\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'question1': 'What is the mountain in the image?',\n",
    "        'question2': 'How tall is the mountain in the image?',\n",
    "        'image': 'https://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/Everest_North_Face_toward_Base_Camp_Tibet_Luca_Galuzzi_2006.jpg/800px-Everest_North_Face_toward_Base_Camp_Tibet_Luca_Galuzzi_2006.jpg'\n",
    "    },\n",
    "    {\n",
    "        'role': 'assistant',\n",
    "        'text': '1. Mount Everest\\n2. 8848m'\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'question1': 'What is the mountain in the image?',\n",
    "        'question2': 'How tall is the mountain in the image?',\n",
    "        'image': 'https://upload.wikimedia.org/wikipedia/commons/thumb/3/3e/MtFuji_FujiCity.jpg/800px-MtFuji_FujiCity.jpg'\n",
    "    }\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
