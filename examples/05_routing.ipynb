{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kuma_llm_utils.llm import (\n",
    "    OpenAIClient, OpenAIWorker, \n",
    "    AnthropicClient, AnthropicWorker,\n",
    "    GoogleAIClient, GoogleAIWorker)\n",
    "from kuma_llm_utils.pipeline import (LLMModule, LLMRouter, Compose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['ANTHROPIC_API_KEY'] = \"ENTER_YOUR_API_KEY\"\n",
    "os.environ['OPENAI_API_KEY'] = \"ENTER_YOUR_API_KEY\"\n",
    "os.environ['GOOGLE_AI_API_KEY']= \"ENTER_YOUR_API_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_engine = OpenAIClient()\n",
    "anthropic_engine = AnthropicClient()\n",
    "google_engine = GoogleAIClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_prompt = '''\\\n",
    "You task is to route the user to the correct engine for their query.\n",
    "Choose your answer from 'default', 'anthropic', or 'google'.\n",
    "Engine list:\n",
    "'openai' - 'default'\n",
    "'anthropic' - 'anthropic'\n",
    "'gemini' - 'google'\n",
    "Query:\n",
    "{question}\n",
    "'''\n",
    "\n",
    "qa_prompt = '''\\\n",
    "What is your name?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_router_worker = OpenAIWorker(\n",
    "    engine=openai_engine,\n",
    "    prompt_template=router_prompt,\n",
    "    generation_params={'model': 'gpt-4o-mini'})\n",
    "openai_qa_worker = OpenAIWorker(\n",
    "    engine=openai_engine, \n",
    "    prompt_template=qa_prompt,\n",
    "    generation_params={'model': 'gpt-4o-mini'})\n",
    "anthropic_qa_worker = AnthropicWorker(\n",
    "    engine=anthropic_engine, \n",
    "    prompt_template=qa_prompt,\n",
    "    generation_params={'model': 'claude-3-5-haiku-latest', 'max_tokens': 1024})\n",
    "gemini_qa_worker = GoogleAIWorker(\n",
    "    engine=google_engine, \n",
    "    prompt_template=qa_prompt,\n",
    "    generation_params={'model': 'gemini-2.0-flash-exp'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Compose([\n",
    "    LLMRouter(\n",
    "        route_llm=openai_router_worker,\n",
    "        job_pipeline={\n",
    "            'default': LLMModule(llm=openai_qa_worker, output_key='answer'),\n",
    "            'anthropic': LLMModule(llm=anthropic_qa_worker, output_key='answer'),\n",
    "            'google': LLMModule(llm=gemini_qa_worker, output_key='answer')\n",
    "        }\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-28 18:41:12,480 - 77   - INFO     - OpenAIClient |  gpt-4o-mini {'request': 1, 'token': 73}\n",
      "2025-01-28 18:41:12,482 - 27   - INFO     - LLMRouter |  route = google\n",
      "2025-01-28 18:41:14,725 - 70   - INFO     - GoogleAIClient |  gemini-2.0-flash-exp {'request': 1, 'input_token': 7, 'output_token': 12}\n",
      "INFO:default:GoogleAIClient |  gemini-2.0-flash-exp {'request': 1, 'input_token': 7, 'output_token': 12}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Use google models', 'answer': 'I am a large language model, trained by Google.\\n'}\n"
     ]
    }
   ],
   "source": [
    "print(await pipeline.generate({'question': 'Use google models'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-28 18:41:15,296 - 77   - INFO     - OpenAIClient |  gpt-4o-mini {'request': 1, 'token': 75}\n",
      "INFO:default:OpenAIClient |  gpt-4o-mini {'request': 1, 'token': 75}\n",
      "2025-01-28 18:41:15,298 - 27   - INFO     - LLMRouter |  route = anthropic\n",
      "INFO:default:LLMRouter |  route = anthropic\n",
      "2025-01-28 18:41:16,349 - 83   - INFO     - AnthropicClient |  claude-3-5-haiku-latest {'request': 1, 'input_token': 12, 'output_token': 24}\n",
      "INFO:default:AnthropicClient |  claude-3-5-haiku-latest {'request': 1, 'input_token': 12, 'output_token': 24}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Use anthropic models', 'answer': \"I'm Claude, an AI created by Anthropic to be helpful, honest, and harmless.\"}\n"
     ]
    }
   ],
   "source": [
    "print(await pipeline.generate({'question': 'Use anthropic models'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-28 18:41:16,859 - 77   - INFO     - OpenAIClient |  gpt-4o-mini {'request': 1, 'token': 74}\n",
      "INFO:default:OpenAIClient |  gpt-4o-mini {'request': 1, 'token': 74}\n",
      "2025-01-28 18:41:16,861 - 27   - INFO     - LLMRouter |  route = default\n",
      "INFO:default:LLMRouter |  route = default\n",
      "2025-01-28 18:41:17,566 - 77   - INFO     - OpenAIClient |  gpt-4o-mini {'request': 1, 'token': 25}\n",
      "INFO:default:OpenAIClient |  gpt-4o-mini {'request': 1, 'token': 25}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Use openai models', 'answer': 'I am called Assistant! How can I help you today?'}\n"
     ]
    }
   ],
   "source": [
    "print(await pipeline.generate({'question': 'Use openai models'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
